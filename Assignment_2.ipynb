{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7877d",
   "metadata": {
    "id": "82e7877d"
   },
   "source": [
    "# Assignmnet 2 (100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b6d20",
   "metadata": {
    "id": "297b6d20"
   },
   "source": [
    "**Name: Kotomi Fukushima** <br>\n",
    "**Email: kof6267@thi.de** <br>\n",
    "**Group:** A <br>\n",
    "**Hours spend *(optional)* :** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f79f88",
   "metadata": {
    "id": "09f79f88"
   },
   "source": [
    "### SMS Spam Detection *(60 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e88d0",
   "metadata": {
    "id": "148e88d0"
   },
   "source": [
    "<p>You are hired as an AI expert in the development department of a telecommunications company. The first thing on your orientation plan is a small project that your boss has assigned you for the following given situation. Your supervisor has given away his private cell phone number on too many websites and is now complaining about daily spam SMS. Therefore, it is your job to write a spam detector in Python. </p>\n",
    "\n",
    "<p>In doing so, you need to use a Naive Bayes classifier that can handle both bag-of-words (BoW) and tf-idf features as input. For the evaluation of your spam detector, an SMS collection is available as a dataset - this has yet to be suitably split into train and test data. To keep the costs as low as possible and to avoid problems with copyrights, your boss insists on a new development with Python.</p>\n",
    "\n",
    "<p>Include a short description of the data preprocessing steps, method, experiment design, hyper-parameters, and evaluation metric. Also, document your findings, drawbacks, and potential improvements.</p>\n",
    "\n",
    "<p>Note: You need to implement the bag-of-words (BoW) and tf-idf feature extractor from scratch. You can use existing python libraries for other tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad12eba",
   "metadata": {
    "id": "fad12eba"
   },
   "source": [
    "**Dataset and Resources**\n",
    "\n",
    "* SMS Spam Collection Dataset: https://archive.ics.uci.edu/dataset/228/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4109920",
   "metadata": {
    "id": "f4109920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for BoW: 0.9820627802690582\n",
      "Classification Report for BoW:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       950\n",
      "           1       0.98      0.90      0.94       165\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    labels = []\n",
    "    messages = []\n",
    "\n",
    "    with open(path, \"r\", encoding = \"utf-8\") as file:\n",
    "        for line in file:\n",
    "            label, message = line.strip().split('\\t')  \n",
    "            labels.append(1 if label == \"spam\" else 0)  \n",
    "            messages.append(message)  \n",
    "    \n",
    "    return labels, messages\n",
    "            \n",
    "\n",
    "\n",
    "def tokenize(text): #remove punctuations and symbols\n",
    "    text = text.lower()\n",
    "    clean_text = \"\" #initialize clean text\n",
    "    for char in text:\n",
    "        if char.isalnum() or char.isspace():\n",
    "            clean_text += char\n",
    "        else:\n",
    "            clean_text += \"\"\n",
    "    clean_text = clean_text.split()\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "def bag_of_words(documents, vocab=None):\n",
    "    tokenized_docs = []\n",
    "    vectors = []\n",
    "    vocab_index = {}\n",
    "\n",
    "\n",
    "    # make a list of tokenized documents \n",
    "    for doc in documents:\n",
    "        tokens = tokenize(doc)\n",
    "        tokenized_docs.append(tokens)\n",
    "            \n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        # make a set of all unique word\n",
    "        for tokens in tokenized_docs:\n",
    "            vocab.update(tokens) \n",
    "        vocab = list(vocab)\n",
    "\n",
    "    for i, word in enumerate(vocab):\n",
    "            vocab_index[word] = i\n",
    "\n",
    "    for tokens in tokenized_docs:\n",
    "        vector = [0] * len(vocab) # create a vector of len(vocab) zeros\n",
    "        for word in tokens:\n",
    "            if word in vocab_index:\n",
    "                vector[vocab_index[word]] += 1\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors, vocab\n",
    "    \n",
    "\n",
    "def tf_idf(documents, vocab=None, vocab_index=None, idf_values=None):\n",
    "    tokenized_docs = []\n",
    "    vocab_index = {}\n",
    "    \n",
    "    # make a list of tokenized documents \n",
    "    for doc in documents:\n",
    "        tokens = tokenize(doc)\n",
    "        tokenized_docs.append(tokens)\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for tokens in tokenized_docs:\n",
    "            vocab.update(tokens)\n",
    "        vocab = list(vocab)\n",
    "\n",
    "        for i, word in enumerate(vocab):\n",
    "            vocab_index[word] = i\n",
    "    \n",
    "        doc_count = [0] * len(vocab)\n",
    "    \n",
    "        for tokens in tokenized_docs:\n",
    "            unique_word = set(tokens)\n",
    "            for token in unique_word:\n",
    "                index = vocab_index[token]\n",
    "                doc_count[index] += 1\n",
    "\n",
    "        N = len(documents)\n",
    "\n",
    "        idf = [0.0] * len(vocab)\n",
    "        for i, df in enumerate(doc_count):\n",
    "            idf[i] = math.log((N + 1) / (df + 1)) + 1\n",
    "\n",
    "    tf_idf_vectors = []\n",
    "    for token in tokenized_docs:\n",
    "        tf = [0] * len(vocab)\n",
    "        for token in tokens:\n",
    "            if token in vocab_index:\n",
    "                index = vocab_index[token]\n",
    "                tf[index] += 1\n",
    "\n",
    "        vector = [0.0] * len(vocab)\n",
    "        for i in range(len(vocab)):\n",
    "            if tf[i] > 0:\n",
    "                vector[i] = (tf[i] / len(tokens)) * idf[i]\n",
    "        tf_idf_vectors.append(vector)\n",
    "\n",
    "    return tf_idf_vectors, vocab, vocab_index, idf_values\n",
    "\n",
    "\n",
    "\n",
    "# split the data into train and test data\n",
    "labels, messages = load_data(\"sms_spam_collection\\SMSSpamCollection\")\n",
    "mes_train, mes_test, lab_train, lab_test = train_test_split(messages, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# train naive bayes classifier\n",
    "model = MultinomialNB()\n",
    "\n",
    "# # use BOW\n",
    "# mes_train_bow, vocab = bag_of_words(mes_train)\n",
    "# mes_test_bow, _ = bag_of_words(mes_test, vocab=vocab)\n",
    "\n",
    "# model.fit(mes_train_bow, lab_train)\n",
    "# predictions = model.predict(mes_test_bow) \n",
    "# print(\"Accuracy for BoW:\", accuracy_score(lab_test, predictions))\n",
    "# print(\"Classification Report for BoW:\\n\", classification_report(lab_test, predictions))\n",
    "\n",
    "\n",
    "\n",
    "# use TF-IDF\n",
    "mes_train_tfidf, vocab, vocab_index, idf_values = tf_idf(mes_train)\n",
    "mes_test_tfidf, _, _, _ = tf_idf(mes_test, vocab, vocab_index, idf_values)\n",
    "\n",
    "model.fit(mes_train_tfidf, lab_train) \n",
    "predictions = model.predict(mes_test_tfidf) \n",
    "print(\"Accuracy for TF-IDF:\", accuracy_score(lab_test, predictions))\n",
    "print(\"Classification Report for TF-IDF:\\n\", classification_report(lab_test, predictions, zero_division = 0))\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jEykmdVPwqPA",
   "metadata": {
    "id": "jEykmdVPwqPA"
   },
   "source": [
    " ### Search Engine *(40 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-CRJj4Ypw1Z2",
   "metadata": {
    "id": "-CRJj4Ypw1Z2"
   },
   "source": [
    "Your boss is impressed with your spam detector and assigns you a new task. As part of improving internal tools, the company wants a search engine that can search through SMS messages and rank them by relevance. Implement the PageRank algorithm from scratch to score each SMS message based on its importance in the document graph.\n",
    "\n",
    "*   Compute TF-IDF vectors for all SMS messages (you can leverage previous implementation)\n",
    "*   Construct a document graph, where each node represents an SMS message and edges are the links between nodes.\n",
    "*  Implement the PageRank algorithm from scratch to assign an importance score to each SMS message based on its position in the document graph.\n",
    "\n",
    "#### Hint : You can use the previous dataset or any dataset from your choice.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G_2IblEnUeju",
   "metadata": {
    "id": "G_2IblEnUeju"
   },
   "source": [
    "## You might need the follwoing formulas for your implementation\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Cosine Similarity Between Two Document Vectors\n",
    "\n",
    "Cosine similarity measures how similar two vectors are based on the angle between them:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_sim}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
    "$$\n",
    "\n",
    "- \\( A \\cdot B \\): Dot product of vectors \\( A \\) and \\( B \\)  \n",
    "- \\( \\|A\\| \\): Euclidean norm (magnitude) of vector \\( A \\)  \n",
    "- \\( \\|B\\| \\): Euclidean norm of vector \\( B \\)\n",
    "\n",
    "**Use case**: Comparing TF-IDF vectors to measure similarity between two messages.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) PageRank of a Node \\( i \\)\n",
    "\n",
    "PageRank estimates the importance of a document based on its connections in a graph:\n",
    "\n",
    "$$\n",
    "PR(i) = \\frac{1 - d}{N} + d \\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( PR(i) \\): PageRank score of node \\( i \\)  \n",
    "- \\( d \\): Damping factor (typically 0.85)  \n",
    "- \\( N \\): Total number of nodes (documents) in the graph  \n",
    "- \\( M(i) \\): Set of nodes that link to node \\( i \\)  \n",
    "- \\( L(j) \\): Number of outbound links from node \\( j \\)  \n",
    "\n",
    "**Interpretation**:  \n",
    "- A document is important if **important documents link to it**.  \n",
    "- The score is split among a node’s outbound links.  \n",
    "- The **teleportation term** $\\text(\\frac{1 - d}{N})$ accounts for random jumps, ensuring stability and fairness.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "111f15ca-147b-45a3-9de1-8e4faf87ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 0: Go until jurong point, crazy.. Available only in bugis n gre... - PageRank: 0.0100\n",
      "Message 1: Ok lar... Joking wif u oni...... - PageRank: 0.0100\n",
      "Message 2: Free entry in 2 a wkly comp to win FA Cup final tkts 21st Ma... - PageRank: 0.0100\n",
      "Message 3: U dun say so early hor... U c already then say...... - PageRank: 0.0100\n",
      "Message 4: Nah I don't think he goes to usf, he lives around here thoug... - PageRank: 0.0100\n",
      "Message 5: FreeMsg Hey there darling it's been 3 week's now and no word... - PageRank: 0.0100\n",
      "Message 6: Even my brother is not like to speak with me. They treat me ... - PageRank: 0.0100\n",
      "Message 7: As per your request 'Melle Melle (Oru Minnaminunginte Nurung... - PageRank: 0.0100\n",
      "Message 8: WINNER!! As a valued network customer you have been selected... - PageRank: 0.0100\n",
      "Message 9: Had your mobile 11 months or more? U R entitled to Update to... - PageRank: 0.0100\n",
      "Message 10: I'm gonna be home soon and i don't want to talk about this s... - PageRank: 0.0100\n",
      "Message 11: SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH1... - PageRank: 0.0100\n",
      "Message 12: URGENT! You have won a 1 week FREE membership in our £100,00... - PageRank: 0.0100\n",
      "Message 13: I've been searching for the right words to thank you for thi... - PageRank: 0.0100\n",
      "Message 14: I HAVE A DATE ON SUNDAY WITH WILL!!... - PageRank: 0.0100\n",
      "Message 15: XXXMobileMovieClub: To use your credit, click the WAP link i... - PageRank: 0.0100\n",
      "Message 16: Oh k...i'm watching here:)... - PageRank: 0.0100\n",
      "Message 17: Eh u remember how 2 spell his name... Yes i did. He v naught... - PageRank: 0.0100\n",
      "Message 18: Fine if thats the way u feel. Thats the way its gota b... - PageRank: 0.0100\n",
      "Message 19: England v Macedonia - dont miss the goals/team news. Txt ur ... - PageRank: 0.0100\n",
      "Message 20: Is that seriously how you spell his name?... - PageRank: 0.0100\n",
      "Message 21: I‘m going to try for 2 months ha ha only joking... - PageRank: 0.0100\n",
      "Message 22: So ü pay first lar... Then when is da stock comin...... - PageRank: 0.0100\n",
      "Message 23: Aft i finish my lunch then i go str down lor. Ard 3 smth lor... - PageRank: 0.0100\n",
      "Message 24: Ffffffffff. Alright no way I can meet up with you sooner?... - PageRank: 0.0100\n",
      "Message 25: Just forced myself to eat a slice. I'm really not hungry tho... - PageRank: 0.0100\n",
      "Message 26: Lol your always so convincing.... - PageRank: 0.0100\n",
      "Message 27: Did you catch the bus ? Are you frying an egg ? Did you make... - PageRank: 0.0100\n",
      "Message 28: I'm back &amp; we're packing the car now, I'll let you know ... - PageRank: 0.0100\n",
      "Message 29: Ahhh. Work. I vaguely remember that! What does it feel like?... - PageRank: 0.0100\n",
      "Message 30: Wait that's still not all that clear, were you not sure abou... - PageRank: 0.0100\n",
      "Message 31: Yeah he got in at 2 and was v apologetic. n had fallen out a... - PageRank: 0.0100\n",
      "Message 32: K tell me anything about you.... - PageRank: 0.0100\n",
      "Message 33: For fear of fainting with the of all that housework you just... - PageRank: 0.0100\n",
      "Message 34: Thanks for your subscription to Ringtone UK your mobile will... - PageRank: 0.0100\n",
      "Message 35: Yup... Ok i go home look at the timings then i msg ü again..... - PageRank: 0.0100\n",
      "Message 36: Oops, I'll let you know when my roommate's done... - PageRank: 0.0100\n",
      "Message 37: I see the letter B on my car... - PageRank: 0.0100\n",
      "Message 38: Anything lor... U decide...... - PageRank: 0.0100\n",
      "Message 39: Hello! How's you and how did saturday go? I was just texting... - PageRank: 0.0100\n",
      "Message 40: Pls go ahead with watts. I just wanted to be sure. Do have a... - PageRank: 0.0100\n",
      "Message 41: Did I forget to tell you ? I want you , I need you, I crave ... - PageRank: 0.0100\n",
      "Message 42: 07732584351 - Rodger Burns - MSG = We tried to call you re y... - PageRank: 0.0100\n",
      "Message 43: WHO ARE YOU SEEING?... - PageRank: 0.0100\n",
      "Message 44: Great! I hope you like your man well endowed. I am  &lt;#&gt... - PageRank: 0.0100\n",
      "Message 45: No calls..messages..missed calls... - PageRank: 0.0100\n",
      "Message 46: Didn't you get hep b immunisation in nigeria.... - PageRank: 0.0100\n",
      "Message 47: Fair enough, anything going on?... - PageRank: 0.0100\n",
      "Message 48: Yeah hopefully, if tyler can't do it I could maybe ask aroun... - PageRank: 0.0100\n",
      "Message 49: U don't know how stubborn I am. I didn't even want to go to ... - PageRank: 0.0100\n",
      "Message 50: What you thinked about me. First time you saw me in class.... - PageRank: 0.0100\n",
      "Message 51: A gram usually runs like  &lt;#&gt; , a half eighth is smart... - PageRank: 0.0100\n",
      "Message 52: K fyi x has a ride early tomorrow morning but he's crashing ... - PageRank: 0.0100\n",
      "Message 53: Wow. I never realized that you were so embarassed by your ac... - PageRank: 0.0100\n",
      "Message 54: SMS. ac Sptv: The New Jersey Devils and the Detroit Red Wing... - PageRank: 0.0100\n",
      "Message 55: Do you know what Mallika Sherawat did yesterday? Find out no... - PageRank: 0.0100\n",
      "Message 56: Congrats! 1 year special cinema pass for 2 is yours. call 09... - PageRank: 0.0100\n",
      "Message 57: Sorry, I'll call later in meeting.... - PageRank: 0.0100\n",
      "Message 58: Tell where you reached... - PageRank: 0.0100\n",
      "Message 59: Yes..gauti and sehwag out of odi series.... - PageRank: 0.0100\n",
      "Message 60: Your gonna have to pick up a $1 burger for yourself on your ... - PageRank: 0.0100\n",
      "Message 61: Ha ha ha good joke. Girls are situation seekers.... - PageRank: 0.0100\n",
      "Message 62: Its a part of checking IQ... - PageRank: 0.0100\n",
      "Message 63: Sorry my roommates took forever, it ok if I come by now?... - PageRank: 0.0100\n",
      "Message 64: Ok lar i double check wif da hair dresser already he said wu... - PageRank: 0.0100\n",
      "Message 65: As a valued customer, I am pleased to advise you that follow... - PageRank: 0.0100\n",
      "Message 66: Today is \"song dedicated day..\" Which song will u dedicate f... - PageRank: 0.0100\n",
      "Message 67: Urgent UR awarded a complimentary trip to EuroDisinc Trav, A... - PageRank: 0.0100\n",
      "Message 68: Did you hear about the new \"Divorce Barbie\"? It comes with a... - PageRank: 0.0100\n",
      "Message 69: I plane to give on this month end.... - PageRank: 0.0100\n",
      "Message 70: Wah lucky man... Then can save money... Hee...... - PageRank: 0.0100\n",
      "Message 71: Finished class where are you.... - PageRank: 0.0100\n",
      "Message 72: HI BABE IM AT HOME NOW WANNA DO SOMETHING? XX... - PageRank: 0.0100\n",
      "Message 73: K..k:)where are you?how did you performed?... - PageRank: 0.0100\n",
      "Message 74: U can call me now...... - PageRank: 0.0100\n",
      "Message 75: I am waiting machan. Call me once you free.... - PageRank: 0.0100\n",
      "Message 76: Thats cool. i am a gentleman and will treat you with dignity... - PageRank: 0.0100\n",
      "Message 77: I like you peoples very much:) but am very shy pa.... - PageRank: 0.0100\n",
      "Message 78: Does not operate after  &lt;#&gt;  or what... - PageRank: 0.0100\n",
      "Message 79: Its not the same here. Still looking for a job. How much do ... - PageRank: 0.0100\n",
      "Message 80: Sorry, I'll call later... - PageRank: 0.0100\n",
      "Message 81: K. Did you call me just now ah?... - PageRank: 0.0100\n",
      "Message 82: Ok i am on the way to home hi hi... - PageRank: 0.0100\n",
      "Message 83: You will be in the place of that man... - PageRank: 0.0100\n",
      "Message 84: Yup next stop.... - PageRank: 0.0100\n",
      "Message 85: I call you later, don't have network. If urgnt, sms me.... - PageRank: 0.0100\n",
      "Message 86: For real when u getting on yo? I only need 2 more tickets an... - PageRank: 0.0100\n",
      "Message 87: Yes I started to send requests to make it but pain came back... - PageRank: 0.0100\n",
      "Message 88: I'm really not up to it still tonight babe... - PageRank: 0.0100\n",
      "Message 89: Ela kano.,il download, come wen ur free..... - PageRank: 0.0100\n",
      "Message 90: Yeah do! Don‘t stand to close tho- you‘ll catch something!... - PageRank: 0.0100\n",
      "Message 91: Sorry to be a pain. Is it ok if we meet another night? I spe... - PageRank: 0.0100\n",
      "Message 92: Smile in Pleasure Smile in Pain Smile when trouble pours lik... - PageRank: 0.0100\n",
      "Message 93: Please call our customer service representative on 0800 169 ... - PageRank: 0.0100\n",
      "Message 94: Havent planning to buy later. I check already lido only got ... - PageRank: 0.0100\n",
      "Message 95: Your free ringtone is waiting to be collected. Simply text t... - PageRank: 0.0100\n",
      "Message 96: Watching telugu movie..wat abt u?... - PageRank: 0.0100\n",
      "Message 97: i see. When we finish we have loads of loans to pay... - PageRank: 0.0100\n",
      "Message 98: Hi. Wk been ok - on hols now! Yes on for a bit of a run. For... - PageRank: 0.0100\n",
      "Message 99: I see a cup of coffee animation... - PageRank: 0.0100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity_calculation(vec_x, vac_y):\n",
    "    dot_product = np.dot(vec_x, vac_y)\n",
    "    norm_x = np.linalg.norm(vec_x)\n",
    "    norm_y = np.linalg.norm(vac_y)\n",
    "    return dot_product / (norm_x * norm_y) if norm_x * norm_y != 0 else 0.0\n",
    "\n",
    "def document_graph(tf_idf_vectors, threshold=0.1):\n",
    "    N = len(tf_idf_vectors)\n",
    "    graph = {}\n",
    "    for i in range(N):\n",
    "        graph[i] = []\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            sim = cosine_similarity_calculation(tf_idf_vectors[i], tf_idf_vectors[j])\n",
    "            if sim > threshold:\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def pagerank(graph, damping_factor=0.85, max_iter=100, tol=1e-6):\n",
    "    N = len(graph)\n",
    "    pr_scores = {i: 1 / N for i in range(N)}  # Initialize with equal scores\n",
    "    teleport = (1 - damping_factor) / N\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        new_pr_scores = {}\n",
    "        \n",
    "        for node in range(N):\n",
    "            rank_sum = 0\n",
    "            for neighbor in graph[node]:\n",
    "                rank_sum += pr_scores[neighbor] / len(graph[neighbor])\n",
    "            new_pr_scores[node] = teleport + damping_factor * rank_sum\n",
    "        \n",
    "        diff = sum(abs(new_pr_scores[node] - pr_scores[node]) for node in range(N))\n",
    "        if diff < tol:\n",
    "            break\n",
    "        \n",
    "        pr_scores = new_pr_scores\n",
    "\n",
    "    return pr_scores\n",
    "\n",
    "def rank_sms_messages(messages):\n",
    "    tf_idf_vectors, vocab, vocab_index, idf_values = tf_idf(messages)\n",
    "    graph = create_document_graph(tf_idf_vectors)\n",
    "    pr_scores = pagerank(graph)\n",
    "    ranked_sms = sorted(pr_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_sms\n",
    "\n",
    "messages = messages[:100]\n",
    "\n",
    "#Compute TF-IDF vectors for all SMS messages\n",
    "tf_idf_vectors, vocab, vocab_index, idf_values = tf_idf(messages)\n",
    "\n",
    "graph = document_graph(tf_idf_vectors, threshold=0.1)\n",
    "\n",
    "pr_scores = pagerank(graph)\n",
    "\n",
    "ranked_sms = sorted(pr_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for idx, score in ranked_sms:  \n",
    "    print(f\"Message {idx}: {messages_subset[idx][:60]}... - PageRank: {score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#reference https://www.datacamp.com/tutorial/python-bag-of-words-model  chatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbc82a",
   "metadata": {
    "id": "55cbc82a"
   },
   "source": [
    "### Additional Experiments *(5 additional points - <span style=\"color: red;\">Optional</span>)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5820d4",
   "metadata": {
    "id": "9b5820d4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
